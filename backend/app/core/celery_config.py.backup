# backend/app/core/celery_config.py - RATE LIMIT FIXED VERSION
"""
Celery Configuration Settings - FIXED RATE LIMITS
Provides centralized configuration for Celery background tasks with async support
"""

from typing import Dict, Any
import os
from kombu import Exchange, Queue


class CeleryConfig:
    """
    Celery configuration class containing all settings for background tasks
    Enhanced with async/await compatibility and FIXED rate limits
    """
    
    # Redis Broker Configuration
    broker_url: str = os.getenv("REDIS_URL", "redis://127.0.0.1:6379/0")
    result_backend: str = os.getenv("REDIS_URL", "redis://127.0.0.1:6379/0")
    
    # Task Serialization
    task_serializer: str = "json"
    result_serializer: str = "json"
    accept_content: list = ["json"]
    
    # Timezone Configuration
    timezone: str = "UTC"
    enable_utc: bool = True
    
    # Broker Connection Settings (FIXED FOR WARNINGS)
    broker_connection_retry_on_startup: bool = True
    broker_connection_retry: bool = True
    broker_connection_max_retries: int = 10
    
    # Worker Configuration (ENHANCED FOR ASYNC)
    worker_prefetch_multiplier: int = 1  # Better for async tasks
    worker_max_tasks_per_child: int = 1000  # Prevent memory leaks
    worker_disable_rate_limits: bool = True
    
    # Task Execution Settings (ASYNC OPTIMIZED)
    task_acks_late: bool = True  # Acknowledge tasks after completion
    task_reject_on_worker_lost: bool = True  # Reject tasks if worker dies
    task_track_started: bool = True  # Track when tasks start
    task_time_limit: int = 300  # 5 minutes max per task
    task_soft_time_limit: int = 240  # 4 minutes soft limit
    
    # Result Backend Settings (ENHANCED)
    result_expires: int = 3600  # Results expire after 1 hour
    result_persistent: bool = True  # Persist results to Redis
    result_compression: str = "gzip"  # Compress results
    
    # Async/AsyncIO Specific Settings (NEW)
    worker_pool: str = "threads"  # Use thread pool for async compatibility
    worker_pool_threads: int = 4  # Number of threads per worker
    
    # Task Routing (ENHANCED)
    task_routes: Dict[str, Dict[str, Any]] = {
        # Data Collection Tasks (High Priority)
        "app.tasks.price_collector.sync_all_prices": {
            "queue": "price_data",
            "priority": 8
        },
        "app.tasks.price_collector.sync_historical_data": {
            "queue": "price_data", 
            "priority": 6
        },
        "app.tasks.price_collector.sync_specific_cryptocurrency": {
            "queue": "price_data",
            "priority": 7
        },
        
        # Discovery and Maintenance Tasks (Medium Priority)
        "app.tasks.price_collector.discover_new_cryptocurrencies": {
            "queue": "scheduling",
            "priority": 4
        },
        "app.tasks.price_collector.cleanup_old_data": {
            "queue": "scheduling",
            "priority": 2
        },
        
        # ML Tasks (Medium Priority) - For future use
        "app.tasks.ml_tasks.*": {
            "queue": "ml_processing",
            "priority": 5
        },
        
        # Scheduler Tasks (Low Priority)
        "app.tasks.scheduler.*": {
            "queue": "scheduling",
            "priority": 3
        }
    }
    
    # Queue Configuration (ENHANCED WITH PRIORITIES)
    task_queues: tuple = (
        # High priority queue for real-time price data
        Queue(
            "price_data",
            Exchange("price_data", type="direct"),
            routing_key="price_data",
            queue_arguments={
                "x-max-priority": 10,
                "x-message-ttl": 300000  # 5 minutes TTL
            }
        ),
        
        # Medium priority queue for ML processing  
        Queue(
            "ml_processing",
            Exchange("ml_processing", type="direct"),
            routing_key="ml_processing",
            queue_arguments={
                "x-max-priority": 8,
                "x-message-ttl": 1800000  # 30 minutes TTL
            }
        ),
        
        # Low priority queue for scheduled tasks
        Queue(
            "scheduling", 
            Exchange("scheduling", type="direct"),
            routing_key="scheduling",
            queue_arguments={
                "x-max-priority": 5,
                "x-message-ttl": 3600000  # 1 hour TTL
            }
        ),
        
        # Default queue
        Queue(
            "default",
            Exchange("default", type="direct"),
            routing_key="default",
            queue_arguments={
                "x-max-priority": 3
            }
        )
    )
    
    # Queue Priority Settings
    task_queue_max_priority: int = 10
    task_default_priority: int = 5
    
    # Monitoring and Logging (ENHANCED)
    worker_send_task_events: bool = True  # Send task events for monitoring
    task_send_sent_event: bool = True  # Send task sent events
    
    # Error Handling (ENHANCED WITH FIXED RATE LIMITS)
    task_annotations: Dict[str, Dict[str, Any]] = {
        # Price collection tasks with FIXED rate limits
        "app.tasks.price_collector.sync_all_prices": {
            "rate_limit": "12/m",  # Max 12 times per minute (FIXED: was working)
            "time_limit": 300,     # 5 minutes max
            "soft_time_limit": 240  # 4 minutes soft limit
        },
        "app.tasks.price_collector.sync_historical_data": {
            "rate_limit": "4/h",   # Max 4 times per hour (FIXED: was working)
            "time_limit": 600,     # 10 minutes max
            "soft_time_limit": 540  # 9 minutes soft limit
        },
        "app.tasks.price_collector.discover_new_cryptocurrencies": {
            "rate_limit": "1/24h",   # Max once per day (FIXED: was 1/d instead of 1/w)
            "time_limit": 900,     # 15 minutes max
            "soft_time_limit": 840  # 14 minutes soft limit
        },
        "app.tasks.price_collector.cleanup_old_data": {
            "rate_limit": "1/24h",   # Max once per day (FIXED: was 1/w which caused error)
            "time_limit": 1800,    # 30 minutes max
            "soft_time_limit": 1740 # 29 minutes soft limit
        }
    }
    
    # Security Settings (ENHANCED)
    worker_hijack_root_logger: bool = False  # Don't hijack root logger
    worker_log_color: bool = False  # Disable colored logs for production
    
    # Performance Optimization (NEW)
    broker_pool_limit: int = 10  # Limit broker connections
    broker_heartbeat: int = 120  # Heartbeat every 2 minutes
    broker_heartbeat_checkrate: float = 2.0  # Check heartbeat every 2 seconds
    
    # Redis-specific optimizations
    redis_max_connections: int = 20  # Max Redis connections
    redis_retry_on_timeout: bool = True  # Retry on Redis timeout
    
    # Beat Schedule (Moved from celery_app.py for better organization)
    beat_schedule: Dict[str, Dict[str, Any]] = {
        # Sync current prices every 5 minutes
        "sync-prices-every-5-minutes": {
            "task": "app.tasks.price_collector.sync_all_prices",
            "schedule": 300.0,  # 5 minutes in seconds
            "options": {
                "queue": "price_data",
                "priority": 8
            }
        },
        
        # Sync historical data every hour
        "sync-historical-every-hour": {
            "task": "app.tasks.price_collector.sync_historical_data",
            "schedule": 3600.0,  # 1 hour in seconds
            "options": {
                "queue": "price_data",
                "priority": 6
            }
        },
        
        # Discover new cryptocurrencies daily at 2 AM
        "discover-new-cryptos-daily": {
            "task": "app.tasks.price_collector.discover_new_cryptocurrencies",
            "schedule": 86400.0,  # 24 hours in seconds
            "options": {
                "queue": "scheduling",
                "priority": 4
            }
        },
        
        # Cleanup old data daily (FIXED: was weekly)
        "cleanup-old-data-daily": {
            "task": "app.tasks.price_collector.cleanup_old_data",
            "schedule": 86400.0,  # 24 hours in seconds (FIXED: was 604800.0 for weekly)
            "options": {
                "queue": "scheduling", 
                "priority": 2
            }
        }
    }
    
    # Development vs Production Settings
    if os.getenv("ENVIRONMENT") == "development":
        # Development-specific settings
        worker_log_level: str = "DEBUG"
        task_always_eager: bool = False  # Don't run tasks synchronously in dev
        task_eager_propagates: bool = True  # Propagate exceptions in eager mode
    else:
        # Production-specific settings
        worker_log_level: str = "INFO"
        task_always_eager: bool = False
        broker_transport_options: Dict[str, Any] = {
            "priority_steps": list(range(10)),  # Priority levels 0-9
            "sep": ":",
            "queue_order_strategy": "priority"
        }


# Additional configuration class for async-specific settings
class AsyncCeleryConfig:
    """
    Additional configuration specifically for async/await support
    """
    
    # AsyncIO Event Loop Policy
    ASYNCIO_EVENT_LOOP_POLICY = "asyncio.DefaultEventLoopPolicy"
    
    # Nest AsyncIO Settings
    NEST_ASYNCIO_ENABLED = True
    
    # Task Handler Settings
    ASYNC_TASK_TIMEOUT = 300  # 5 minutes default timeout
    ASYNC_TASK_RETRY_DELAY = 60  # 1 minute retry delay
    ASYNC_TASK_MAX_RETRIES = 3  # Maximum retries for async tasks
    
    # Event Loop Management
    CREATE_NEW_LOOP_ON_CLOSED = True
    USE_NESTED_LOOPS = True
    
    @classmethod
    def apply_async_settings(cls, celery_app):
        """
        Apply async-specific settings to Celery app
        
        Args:
            celery_app: Celery application instance
        """
        # Enable nest_asyncio if not already enabled
        try:
            import nest_asyncio
            nest_asyncio.apply()
        except ImportError:
            pass  # nest_asyncio not available
        
        # Set asyncio policy if available
        try:
            import asyncio
            if hasattr(asyncio, 'WindowsSelectorEventLoopPolicy'):
                # Use selector event loop on Windows
                asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
        except Exception:
            pass  # Keep default policy


# Export configuration classes
__all__ = ['CeleryConfig', 'AsyncCeleryConfig']